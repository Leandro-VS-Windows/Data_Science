{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply ML to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos trabalhar com uma tecnica que faz parte do conjunto __Natural language Processing (NLP)__ chamado _Sentiment Analysis_ , além de entender como usar algoritmos de ML para classificar documentos baseados na sua popularidade. Para isso utilizaremos um dataset do __Intert Movie Database (IMDB)__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Preparing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Dataset consiste dos reviews de 50.000 filmes classificados em negativos e positivos, onde positivos significa que o review recebeu mais que 6 estrelas no IMDB e um review negativo se recebeu menos que 5 estrelas.<br>\n",
    "Os arquivos deste de data consistem de dois diretórios um para o conjunto de treino e outro para o conjunto de teste e em cada um deles mais dois diretórios, um para reviews positivos e outro para negativos. Todos os arquivos estão em txt, iremos então ler estes arquivos e add eles a uma objeto DataFrame do pandas. <br>\n",
    "Para essa tarefa pode levar algum tempo na maioria dos computadores, devido a natureza da operação e a quantidade de iterações feitas. Sendo assim utilizaremos a lib __PyPrind - Python Progress Indicator__ para acompanhar o progresso dessa operação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libs\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:11:14\n"
     ]
    }
   ],
   "source": [
    "#Path dos arquivos do dataset\n",
    "basepath = './aclImdb_v1/'\n",
    "\n",
    "#Labels dos diretórios\n",
    "labels = {'pos':1, 'neg':0}\n",
    "\n",
    "#Barra de progresso\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            \n",
    "            pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset criado possui as labels em ordem dos diretórios acessados, afim de facilitar e tornar mais coerente o split dos dados em Train/Test iremos embaralhar o dataset fazendo uso da função _permutation_ da _np.random_ . Além disso por conveniência iremos converter o dataframe para um arquivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corrigindo os nomes das colunas\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Cria um gerador com seed(semente inicializadora) zero, ou seja, a cada chamada os mesmos \"numero aleatórios\" \n",
    "#aparecerão.\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>I was \"turned on\" to this movie by my flight i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>Dresden had great expectations because of its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45519</th>\n",
       "      <td>Some people seem to think this was the worst m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25747</th>\n",
       "      <td>This movie has a lot of comedy, not dark and G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42642</th>\n",
       "      <td>Jack Black is an annoying character.This is an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "11841  I was \"turned on\" to this movie by my flight i...          1\n",
       "19602  Dresden had great expectations because of its ...          0\n",
       "45519  Some people seem to think this was the worst m...          0\n",
       "25747  This movie has a lot of comedy, not dark and G...          1\n",
       "42642  Jack Black is an annoying character.This is an...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words é uma forma de representar texto em um vetor numerico, basicamente podemos dizer que:\n",
    "* Criaremos um vocabulario de unico de tokens (por exemplo palavras), de todo o cunjunto de dados\n",
    "* Construiremos um vetor para cada documento contendo a quantidade de vezes de ocorrência de cada palavra \n",
    "<br>\n",
    "\n",
    "Como as palavras unicas (tokens) em cada documento representam apenas um pequeno subset do conjunto bag-of-words, os vetores consistiram principalmente de zeros, e por isso são chamados de vetores (ou matriz) esparsos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando palavras em um vetor de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a construção de tal vetor podemos utilizar a classe _CountVectorizer_ da lib scikit-learn, tal classe toma uma array de texto, podendo ser documentos ou frases, e construi um modelo de bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "\n",
    "docs = np.array(['The sun is shining', \n",
    "                 'The weather is sweet', \n",
    "                 'The sun in shining and the weather is sweet'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao chamar o metódo _fit_transform_ nós construimos um vocabulario que será nossa bag-of-words e transformamos as frases passadas em vetores esparsos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 2, 'shining': 3, 'weather': 7, 'sweet': 5, 'in': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver o vocabulario é um dicionario que mapeia as palavras unicas a um indice inteiro. <br><br>\n",
    "\n",
    "Veremos agora o vetor criado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 1 0]\n",
      " [0 0 1 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada posição (indice) no vetor de features, corresponde ao valor inteiro armazenado no dicionario de itens _CountVectorizer_ , por exemplo a primeira feature do vetor cujo indice é 0 assemelha-se a palavra 'and' que ocorre apenas no ultimo documento. Os valores do vetor de features são chamados de __raw term frequencies:__ $tf(t,d)$ , o numero de vezes que um ter $t$ ocorre em um documento $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Note:___ A sequencia de itens da bag-of-words criada é chamada de modelo __1-gram__ ou __unigram__ onde cada item ou token do vocabulario corresponde a uma unica palavra. De maneira mais geral, uma sequencia de itens em __NLP__ - palavras, letras ou simbolos - são chamados de __n-grams__ . A escolha do numero n do modelo n-gram depende da aplicação. Como exemplo, podemos dizer então que:\n",
    "* __1-gram:__ \"the\", \"sun\", \"is\", \"shining\"\n",
    "* __2-gram:__ \"the sun\", \"sun is\", \"is shining\"\n",
    "\n",
    "A classe _CountVectorizer_ do scikit-learn permite utilizar facilmente modelos n-grams através do parametro _ngram_range_ , onde 1-gram é usado por padrão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação da relevância das palavras por meio da frequência do termo inversa a frequência por documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No contexto deste projeto, assim como em muitos outros, encontraremos palavras que aparecem em varios documentos sendo das duas classes presentes (positiva e negativa) e este tipo de ocorrência não contem muitas informações descriminatórias uteis. Iremos então estudar uma tecnica útil  chamada __\"Term frequency-inverse document frequency\" (tf-idf)__ ( frequência do termo–inverso da frequência nos documentos) , que pode ser usada para diminuir o peso dessas palavras que ocorrem frequentemente no vetor de features, ou seja, diminui o peso da palavra de um documento com relação a todos os outros documentos. <br>\n",
    "Tf-idf pode ser definido como: \n",
    "\\begin{equation*}\n",
    "    tfidf(t,d) = tf(t,d) * idf(t,d)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onde $tf(t,d)$ é a frequência de um termo em um documento visto anteriormente e $idf(t,d)$ é a frequencia de documento inversa, e é calculada como:\n",
    "\n",
    "\\begin{equation*}\n",
    "    idf(t, d) = log[\\frac{n_d}{1+df(d,t)}]\n",
    "\\end{equation*}\n",
    "\n",
    "Onde, $n_d$ é o numero total de documentos e $df(d,t)$ é o numero de documentos $d$ que contem o termo $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O termo $1$ no demoninador serve para não haver divisão por zero caso a função $df$ retorne zero. O $log$ é usado para garantir que baixas frequencia de documentos não sejam dado muito peso.<br>\n",
    "<br>\n",
    "Esta transformação esta implementada no _scikit-learn_ como uma classe, __TfdifTransformer__ , que recebe os _raw terms frequencies_ do _CountVectorizer_ e então transforma-os com tf-idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.   0.43 0.   0.   0.56 0.43 0.56]\n",
      " [0.41 0.41 0.24 0.31 0.31 0.31 0.48 0.31]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\"\"\"\n",
    "norm: Normalização para cada linha do output. l2 siginica Soma dos quadrados = 1, ou seja, o angulo entre\n",
    "      dois vetores é 1.\n",
    "\n",
    "use_idf: Determina a utilização do Idf\n",
    "\n",
    "smooth_idf: Previni divisões por zero add 1 aos pesos das frequencias dos documentos\n",
    "\"\"\"\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TfidTransformer__ após fazer o produtor vetorial entre $df$ e $idf$, através do parametro _norm_ normaliza o resultado, isso garante uma uniformidade nos valores além de impedir uma possível divisão por zero. Neste caso utilizamos a normalização por __L-2 normalization__ (l2) que consite da divisão do vetor candidato a normalização pela sua norma L2:\n",
    "\n",
    "\\begin{equation*}\n",
    "    V_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}} = \\frac{v}{\\sqrt{\\sum_{i=1}^n v_i^2}}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza dos dados de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo após construir nossa bag-of-words consiste em limpar nossos dados para remover os caracteres indesejados. Para ilustrar a importancia disso, vejamos um exemplo de como estão os textos de um dos nossos documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" play it, enjoy it, love it. It's PURE BRILLIANCE.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Há muitos simbolos de pontuação, assim como podem haver marcações de HTML em outros documentos. Sendo assim retirarei todos esses pontos e marcações, com excessão de possiveis emoticons como \":)\" que podem ser uteis para a analise de sentimetno. <br>\n",
    "Para esta tarefa utilizaremos a biblioteca de expressões regulares do Python __Python Regular Expression (regex)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para remover as pontuações e marcadores de html\n",
    "def preprocessor(text):\n",
    "    \n",
    "    #regex para remover marcadores html\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    \n",
    "    #armazenando os caracteres de emoticons para usa-los futuramente\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    \n",
    "    #removendo todas as \"não-palavras\" e convertendo todo o texto para minusculo\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Não estamos levando em conta a _capitalização_ das palavras, ou seja, a ordem em que elas aparencem na frase, assumindo assim que esta informação não tem relevância para esta analise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Para saber mais sobre expressões regulares:\n",
    "* Tutorial (muito bom) da Goggle: https://developers.google.com/edu/python/regular-expressions\n",
    "* Documentação Python: https://docs.python.org/3.6/library/re.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando de a função preprocessor funciona adequadamente\n",
    "preprocessor(df.loc[0, 'review'][-50:])\n",
    "\n",
    "preprocessor(\"</a>This :) is :(a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esta funcionando bem, vamos então aplicar a função a todos os nossos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processando Documentos em Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma maneira de empregar essa tecnica de _tekenize_ documentos, consiste em dividir o documentos em palavras individuais, separando-as pelos espaços em branco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Função tokenize\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "#Testando\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma outra tecnica possível é a chamada __Word Stemming__ , que consiste em transformar uma palavra em sua \"forma raiz\", ou seja, reconhecendo que aquela é uma palavra derivada e através do seu sufixo encontrando a sua forma raiz. Esta tecnica foi criada por _Martin F. Porter_ que a chamou de __Porter Stemmer Algorithm__ em 1979. Anos mais tarde foi criada a [__Natural Language Toolkit (NLTK)__](http://www.nltk.org/) que implementa este algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Porter Stemmer from NLTK\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "#Testando\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos o novo tokenizador retorna palavras derivadas (\"running\") para sua forma raiz (\"run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Porter Stemmer ja é considerado bem antigo e há outros algoritmos mais modernos para fazer a mesma tarefa de forma mais eficiente como o __Snowball Stemmer__ e o __Lancaster Stemmer__, ambos também implementados na lin _NLTK_ .<br>\n",
    "Existe uma outra tecnica bem mais agressiva que a de _Stemming_ e que também minimiza os erros gerados, como de palavras que não existem, chamado de __Lemmatization__ que retorna as palvras para sua forma caninca, ou seja, sua forma raiz mas correta gramaticalmente, tal tecnica é bem mais dificil de implementar e alguns estudos apontam pouco impacto na performance em modelos de classificação de texto - ver: (Influence of Word Normalization on Text Classification, Michal Toman, Roman Tesar, and Karel Jezek, Proceedings of InSciT, pages 354–358, 2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop-words são palavras muito comuns em textos que não necessariamente contribuem para uma classificação do texto, são palavras de conexão e de ligação entre as frases, por exemplo do inglês: _is, and, has, like_ <br>\n",
    "A remoção dessas stop-words é útil quando estamos trabalhando com a raiz das palavras ou termos que tenham sua frequencia normalizada ao invés de Tfidf em que ja é diminuido a frequencia de ocorrência das palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta tarefa iremos utilizar um conjunto de dados contendo 127 stop-words (do Inglês) disponíveis na propria lib _NLTK_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/leandro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#Fazendo o download da própria lib do conjunto de stop-words\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando as stop-words baixadas\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a  lot')[-10:] if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As stop-words foram removidas e o texto foi transformado para sua forma raiz e dividido em palavras unicas__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando um modelo de Regressão Logistica para Classificação de Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramento vamos dividir nosso conjunto de dados (já limpo) em conjunto de treino e de test; dividiremos em 50% para cada conjunto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20448,) (20448,) (29553,) (29553,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos agora o __GridSearchCV__ para fazer uma busca pelos melhores parametros para nosso modelo de regressão logistica, usando o metodo de validação cruzada com 5-folds estratificados. <br>\n",
    "Além disso utilizaremos a biblioteca pipeline que facilita todo o trabalho de transformações e mudanças de parametros nos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libs\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizando a frequencia das palavras no documento\n",
    "#Parametros com None por que os documentos ja foram limpos\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "#Parametros a serem testamos\n",
    "\"\"\"\n",
    "Chaves:\n",
    "    ngram_range: Range de palavras q serão consideradas unicas (1-gram, 2-gram, etc...), referente a função de \n",
    "                 analise de frequencia de termos\n",
    "    stop_words:  lista com as palavras de ligação e/ou sem valor semantico\n",
    "    tokenizer:   Função que fara o split do texto e transformará as palavras para sua raiz\n",
    "    penalty:     Método de normalização dos resultados do modelo\n",
    "    C:           Inverso da \"força\" de regularização, valores menores indicam uma regularização maior dos dados\n",
    "    use_idf:     Habilita o rebalanceamento inverso da frequencia de termo por documento\n",
    "    norm:        Normalização do produto vetorial (usado na analise de frequencia de termos)\n",
    "\"\"\"\n",
    "param_grid = [{'vect__ngram_range':[(1,1)], \n",
    "               'vect__stop_words':[stop, None], \n",
    "               'vect__tokenizer':[tokenizer, tokenizer_porter], \n",
    "               'clf__penalty':['l1', 'l2'], \n",
    "               'clf__C':[1.0, 10.0, 100.0]}, \n",
    "             \n",
    "              {'vect__ngram_range':[(1,1)], \n",
    "              'vect__stop_words':[stop, None], \n",
    "              'vect__tokenizer':[tokenizer, tokenizer_porter], \n",
    "              'vect__use_idf':[False], \n",
    "              'vect__norm':[None], \n",
    "              'clf__penalty':['l1', 'l2'], \n",
    "              'clf__C':[1.0, 10.0, 100.0]}\n",
    "             ]\n",
    "\n",
    "#Pipeline\n",
    "\"\"\"\n",
    "Parametros: Passados apartir de um dicionario com chaves e valores que serão testados nos modelos\n",
    "    Lista de tuplas:\n",
    "        Primeiro argumento: (Nome, Transformação) - Transformação sobre os dados do modelo\n",
    "        Segundo argumento: (Modelo) - Modelo a ser implementado\n",
    "\"\"\"\n",
    "lr_tfidf = Pipeline([('vect', tfidf), \n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "#GridSearchCV\n",
    "\"\"\"\n",
    "Parametros (respectivamente):\n",
    "    estimator (lr_tfidf): interface que relacionará o modelo aos parametros que serão testados, neste caso\n",
    "                          a função pipeline, mas poderiam ser outros modelos com parametros diferentes\n",
    "                          \n",
    "    param_grid:           Dicionarios com o nomes dos parametros como chaves (string) e uma lista como valores\n",
    "    \n",
    "    scoring:              Metrica para avaliação do modelo\n",
    "    \n",
    "    cv:                   Um inteiro como gerador das divisões para uma validação cruzada\n",
    "    \n",
    "    verbose:              Quanto maior o valor mais mensagens exibidas durante a busca\n",
    "    \n",
    "    n_jobs:               Números de tarefas que serão executadas em paralelo, afeta os nucleos do processador \n",
    "                          que serão usados. Utiliza '-1' fará uso de todos os nucleos, acelerando o processo\n",
    "    \n",
    "\"\"\"\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/home/leandro/.conda/envs/Data_Science/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 86.7min\n",
      "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 110.8min finished\n",
      "/home/leandro/.conda/envs/Data_Science/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_word...\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7f2188755440>,\n",
       "                                              <function tokenizer_porter at 0x7f215d03f170>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aplicando o gridsearchcv nos dados de treino\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___A operação acima levou em torno de 45min para ser completada!!!___\n",
    "\n",
    "Na Busca do GridSearchCV utilizamos o __TfidfVectorizer__ pois ele faz o mesmo que as outras duas classes usadas anteriormente, CountVectorizer e TfidfTransformer. <br>\n",
    "_A saber:_\n",
    "* __CountVectorizer:__ Convert a collection of text documents to a matrix of token counts\n",
    "This implementation produces a sparse representation of the counts using\n",
    "scipy.sparse.csr_matrix.\n",
    "\n",
    "* __TfidfTransformer:__ Transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "* __TfidfVectorizer:__ Convert a collection of raw documents to a matrix of TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O ___param_grid___ utilizado consiste de dois dicionarios como parametros. No primeiro utilizamos as configurações padrões do _ TfidfVectorizer_ (use_idf=True, smooth_idf=True, and norm='l2') para calcular a tf-idf; no segundo dicionario mudamos alguns parametros (use_idf=False, smooth_idf=False, and norm=None)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printando os melhores parametros encontrado pelo GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7f2188755440>}\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos então que, por exemplo, os melhores parametros são a função de tokenização simples (sem a implementação Porter), não se faz necessário as stop-words e usando a tf-idf em combinação com LogisticRegression com L2-regularization, com uma força de regularização de C=10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o melhor modelo encontrando pelo grid search, veremos a média da accuracy após uma validação cruzada de 5-folds nos dados de treino e accuracy da classificação no conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "#Accuracy nos dados de treino\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "#Treinando o modelo com os melhores parametros\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "#Accuracy nos dados de test\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado revela que o modelo de Machine Learning consegue prever quando uma avaliação de um filme é positiva ou negativa com __~90% de acurácia__ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with bigger data – online algorithms and out-of-core learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como visto na analise anterior, trabalhar com analise de sentimento de um conjutno de dados tão grande (50.000 reviews) pode ser custoso computacionalmente. Em muitas aplicações do mundo real não é incomum lidar com banco de dados maiores ainda e que as operações sobre eles podem exceder facilmente a capacidade de memória dos cumputadores. Para resolver este problema estudaremos uma tecnica chamada de Out-Of-Core Learning , que nos permite trabalhar com banco de dados grandes através de pequenos fits do modelo classificador em \"parcelas\" menores do banco de dados, de forma incremental.\n",
    "\n",
    "Para tal tarefa utilizaremos a função partial_fit do SGDClassifier do scikit-learn, para \"transmitir\" os documentos diretamente do local drive, e treinar um modelo de Logistic Regression usando pequenos pacotes de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro de tudo iremos definir uma função _tokenizer_ para limpar os documentos não processados do __movie_data.cvs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libs\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "    text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora definiremos uma função __stream_docs__ para ler e retornar um documento por vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) #skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I was \"\"turned on\"\" to this movie by my flight instructor and now I wonder how the heck it was out there for nearly five years before I finally discovered it. If you have any love of flying at all, especially an attachment to the planes of WWII, this is an absolute must see, vastly superior to the pathetic \"\"Pearl Harbor\"\" and up there in rivalry with the famed \"\"Battle of Britain\"\" filmed more than thirty years ago. There are moments when you feel as if you are flying wingman, literally dodging the shell casings of your leader as you roll in on a Me 109 or He 111. <br /><br />As an historian this film deeply touched me as well for it is about the plight endured by tens of thousands of gallant Poles, Hungarians, Slovaks and Czechs who in 1939-1940 fled their homelands, made it to England, fought with utmost bravery for the survival of western civilization, and then were so callously abandoned by \"\"us\"\" after the war when they were arrested by the communists upon their return to their native lands. I have stood atop Monte Cassino in Italy and was moved to tears by the cemetery for the Polish troops that stormed that mountain that British and Americans could not take. I have traveled as well to Prague (the most beautiful of cities) and studied their history. Their story of abandonment, I believe, should be a lesson to us even today about obligations to gallant allies. <br /><br />But back to the film. If you love flying, see this. If you are interested in the aircraft of WWII most definitely see it. Without doubt the most brutal, direct, and frightfully swift air combat scenes ever replicated for film. And yes, if you even are seeking a touching romance, there is that as well in heartbreaking detail.<br /><br />Bill Forstchen Professor of History Co-owner of a WWII replica \"\"warbird\"\" P-51 Mustang \"\"Gloria Ann\"\"\"',\n",
       " 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testando a função\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos agora definir uma função para retornar apenas uma porção do documento selecionado, determinado por um parametro _size_ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A função next() itera sobre uma lista, retornando o proximo elemento, a cada chamada ele pula \n",
    "#o elemento anteriormente retornado e vai para o proximo\n",
    "\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não podemos usar função _CountVectorizer_ por ela precida criaf todo um vocabulario na memória, e nem mesmo a _TfidfVectorizer_ por que ela precisa manter na memoria todos os vetores de vocabularios para operar sobre eles, e nossas função criadas aqui operam apenas uma vez sobre cada linha do documento. Entretanto há no scikit-learn uma outra classe para fazer a vetorização para processamento de texto chamada __HashingVectorizer.HashingVectorizer__ , trata dados independentes e utiliza tecnicnas de _hashing_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, \n",
    "                         preprocessor=None, tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, n_iter_no_change=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicializamos então o HashingVectorizer com um numero grande de features, isso fará com que diminua a chance de \" _colisões de hash_ \", porem aumenta o numero de coeficientes do modelo de regressão logistica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vamos agora, de fato inicia o método de Out-Of-Core Learning:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:34\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos nossa barra de medição(pbar) com 45 interações e no _loop_ iteramos sobre 45 mini-baches (porções pequenas dos documentos), onde cada mini-bache consiste de 1000 documentos, ou seja teremos então 45000 documentos processados ao final. Os outros 5000 usaremos para avaliar o mdoelo treinado com _partial_fit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "\n",
    "#Add os 5k documentos restantes aos conjuntos de testes\n",
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "\n",
    "#Vetorizando os documentos co HashingVectorizer\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "#Print do score\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tivemos uma acuracia ligeiramente menor __(~88%)__ do que o modelo treinado com GridSearchCV, porem o ganho computacional foi enorme, em menos de 1min o código foi executado, provando assim que o método de Out-of-Core Learning é bem eficiente e entrega resultados equivalentes aos outros.\n",
    "<br><br>\n",
    "Podemos por fim atualizar nosso modelo, treinando-o com os 5k documentos restantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalizado o Treinamento incremental do modelo\n",
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Hoje em dia a um algoritmo mais moderno para criar uma _bag-of-words_ , é um modelo chamado de [__word2vec__](https://code.google.com/p/word2vec/) , um algoritmo que o Google disponibilizou em 2013. Word2vec é um modelo de aprendizagem não-supervisionada baseado em redes neurais com aprendizado automático sobre o relacionamento das palavras. A idéia por trás deste algoritmo é colocar as palavras com significados similares dentro de _clusters_ similares, e através de um \"espaço vetorial inteligente\", o modelo reproduz certas palavras usando uma simples matematica de vetores, por exemplo: _king - man + woman = queen_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializando o modelo out-of-core, para a próxima seção que envolve o deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pickle__ serve para serializar o modelo, ou seja, \"salvar\" o estado atual do modelo para futuramente poder usa-lo sem a necessidade de retreina-lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Criação dos diretórios:\n",
    "            movieclassifier: armazenará os arquivos e dados da aplicação web\n",
    "            pkl_objects: Um subdiretório para salvar os objetos python serializados\n",
    "\"\"\"\n",
    "dest = os.path.join('movieclassifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "    \n",
    "\"\"\"\n",
    "O método Dump irá serializar o modelo de logistic regression treinado anteriormente, assim como as stop-words\n",
    "usadas da lib NLTK, desta forma não será preciso instalar a NLTK no servidor.\n",
    "\n",
    "O primeiro argumento refere-se ao objeto que será serializado.\n",
    "O segundo argumento é um arquivo aberto onde o objeto Pyhton será escrito\n",
    "    Através do argumento 'wb' na função open, o arquivo é aberto em modo binario pelo pickle\n",
    "    O argumento protocol=4 escolhe o ultimo e mais eficiente protocolo implementado pelo pickle apartir da versão\n",
    "        do Python 3.4\n",
    "\"\"\"\n",
    "pickle.dump(stop, \n",
    "                open(os.path.join(dest, 'stopwords.pkl'), 'wb'), \n",
    "                protocol=4)\n",
    "\n",
    "pickle.dump(clf, \n",
    "               open(os.path.join(dest, 'classifier.pkl'), 'wb'), \n",
    "               protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Diretório movido para a Pasta do chapter 9__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo de Regressão Logistica treinado possui diversos numpy arrays e uma maneira mais eficiente de serializar numpy arrays é com a biblioteca __joblib__ , porem por questão de compatibilidade com o servidor que iremos utilizar para hospedar a aplicação utilizaremos o __pickle__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem de Tópicos com \"Latent Dirichlet Allocation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelagem de tópicos descreve uma ampla gama de tarefas sobre documentos de texto sem labels. Um tipico exemplo é a categorização de documentos dentro de um grande conjunto de artigos de jornais, onde não sabemos a quais categorias os artigos pertencem. Em modelgame de tópicos buscamos determinar a qual categoria os artigos pertencem - por exemplo, esportes, finanças, news, politica, etc... No contexto de problemas de ML envolvendo categorização podemos considerar a _Modelagem de Tópicos_ como um problema de _Clusterização_ , uma subcategoria do Aprendizado Não-Supervisionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos estudar a técnica de modelagem de tópico chamada __Latent Dirichlet Allocation__ comumente abreviada para __LDA__ , porem não devemos confundir com uma outra técnica: __Linear Discriminant Analysis__ que trata-se de uma aprendizagem supervisionada de redução de dimensionalidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA com Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos utilizar o LDA do scikit-learn para decompor nosso dataset de review de filmes e categoriza-lo em diferentes topicos. Restringimos, inicialmente, a analise para 10 tópicos, porem é possível experimentar outros hiperparametros do algoritmo para explorar outros topicos.<br>\n",
    "Então vamos começar do começo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was \"turned on\" to this movie by my flight i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dresden had great expectations because of its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some people seem to think this was the worst m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie has a lot of comedy, not dark and G...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jack Black is an annoying character.This is an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  I was \"turned on\" to this movie by my flight i...          1\n",
       "1  Dresden had great expectations because of its ...          0\n",
       "2  Some people seem to think this was the worst m...          0\n",
       "3  This movie has a lot of comedy, not dark and G...          1\n",
       "4  Jack Black is an annoying character.This is an...          0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load dataset\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criaremos agora uma matrix de _bag-of-word_ com o __CountVectorizer__ para dar como entrada no __LDA__ . Por conveniencia iremos utilizar o stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\"\n",
    "Parametros:\n",
    "    max_df: Cria um vocabulario ignorando os termos com uma frequencia maior que o limite determinado\n",
    "    \n",
    "    max_features: Se não for None, cria um vocabulario que considera apenas os principais max_features, ordenados\n",
    "                  por termos de frequencia nos documentos\n",
    "\"\"\"\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos a frequência maxima de palavras do documento a ser considerada como 10% (max_df=.1), para excluir palavras que ocorrem com muita frequencia nos documentos. A lógica por tras dessa remoção é que palavras que ocorrem com uma frequencia alta em todos os documentos, possivelmente não contribuem para uma determinação da categoria do documento, por serem palavras genericas. Também reduzimos o numero de palavras para ser consideradas para as 5000 mais frequentes (max_features=5000), para limitar a dimensionalidade do dataset e melhorar a perfomance do LDA. Entretando tanto o _max_def_ quanto _max_features_ são hiperparametros escolhidos arbitrariamente e ambos podem ser tunados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementando o LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Instanciando o modelo\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch')\n",
    "\n",
    "#Fit\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método __LDA__ configurado da maneira acima,  leva em torno de 5min para rodar em pcs normais "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o _learning_method = 'batch'_ deixamos o lda fazer sua estimativa baseado em todo o conjunto de treino disponível (matrix de bag-of-words) em uma iteração, o que torna a execução mais lenta do que seria com o método _'online'_ porem desta forma teremos um resultado mais acurado. <br>\n",
    "<br>\n",
    "__OBS:__ A escolha do metodo de aprendizagem 'online' é semelhante ao 'mini-batch' criado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos verificar agora a matrix gerada pelo LDA contendo a importancia das palavras (neste caso 5000) para cada um dos 10 topicos em ordem crescente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father girl women\n",
      "Topic 3:\n",
      "american dvd music tv early\n",
      "Topic 4:\n",
      "audience human cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex gore blood\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series war episode episodes season\n",
      "Topic 9:\n",
      "book version original effects read\n",
      "Topic 10:\n",
      "action guy fight guys music\n"
     ]
    }
   ],
   "source": [
    "#Listando as 5 palavras mais importantes para distinguir cada topico\n",
    "#Lembrando que as palavras são retornadas em ordem crescente, então para exibir um Top 5 devemos reverter o vetor\n",
    "\n",
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort() [:-n_top_words - 1 :-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseado nas palavras mais importantes de cada tópico, podemos assumir que o LDA identificou os seguinte topicos:\n",
    "<ol>\n",
    "    <li>Filmes ruins em geral</li>\n",
    "    <li>Filmes sobre familias</li>\n",
    "    <li>Shows de TV</li>\n",
    "    <li>Filmes sobre Arte</li>\n",
    "    <li>Filmes policiais</li>\n",
    "    <li>Filmes de Terror</li>\n",
    "    <li>Filmes de Comédia</li>\n",
    "    <li>Séries de guerra (talvez)</li>\n",
    "    <li>Filmes baseados em livros</li>\n",
    "    <li>Filmes de ação</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos testar se esses tópicos identificados pelo modelo fazem algum sentido exibindo, por exemplo, 3 reviews de filmes que seriam da categoria _Terror_ (Topic=6, mas com indice igual a 5 por ser um vetor) e verificar se os reviews condizem com filmes de terror:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horro Movie #1:\n",
      "Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n",
      "\n",
      "Horro Movie #2:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horro Movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorro Movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparentemente os reviews condizem com a categoria!!! Very nice!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
