{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaline é muito semelhante ao percetron, muda principalmente o metodo _fit_ que agora atualiza os pesos minimizando a função custo através do gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineGD(object):\n",
    "    \"\"\"Perceptron Classificador\n",
    "    \n",
    "    Parametros\n",
    "    ------------\n",
    "    eta: float\n",
    "        Taxa de Aprendizagem (varia entre 0.0 e 1.0)\n",
    "    n_iter: int\n",
    "        Passos sobre o dataset de treinamento (epochs)\n",
    "    random_state: int\n",
    "        Numero aleatório gerador dos seeds para inicialização com pesos randomicos\n",
    "    \n",
    "    Atributos\n",
    "    -----------\n",
    "    w_: Vetor 1D\n",
    "        Pesos depois do fitting\n",
    "    errors_: list\n",
    "        Numeros de Classificações erradas em cada epoch\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit dos dados de treino\n",
    "        \n",
    "        Parametros\n",
    "        ------------\n",
    "        X: {array-like}, shape = [n_samples, n_features]\n",
    "            Vetor de treino, onde n_sample é o numero de amostras e\n",
    "            n_features o numero de fetures\n",
    "        y: array-like, shape = [n_samples]\n",
    "            Valores Target.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self: objeto\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_sate)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1+ X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            outut = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            #X.T.dot(errors) --> Matriz de multiplacação entre a matrix das features e o vetor erro\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors) # Calcula o gradiente para pesos de 1 a m\n",
    "            self.w_[0] += self.eta * errors.sum() #Calcula o gradiente sobre toda a amostra de treino para o bias\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost) #Para verificar onde o algoritmo converge\n",
    "        return self\n",
    "    \n",
    "    def n_input(self, X):\n",
    "        \"\"\"Calcula o net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    #Apenas uma função identidade para ilustrar a linearidade da ativação\n",
    "    #Em casos mais complexos, como de não lineraridade, esta função defini-se de forma diferente\n",
    "    def activation(self, X):\n",
    "        \"\"\"Computa a ativação linear\"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Retorna a label da classe\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em principio um dos fatores que determinaram a qualidade dos resultados do algoritmo são as escolhas para os hiperametros (eta e epochs). Vamos experimentar dois valores diferentes para o eta e plotar a função custo em função do numero de epochs para ver o comportamento do Adaline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-18e3d3c0f16d>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-18e3d3c0f16d>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    ax.[0].set_xlabel('Epochs')\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Plot da função custo vs Epochs, para dois valores do eta(taxa de aprendizagem)\n",
    "fig, ax = plot.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "\n",
    "#eta=0.01\n",
    "ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X,y)\n",
    "ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-Squared-Error)')\n",
    "ax[0].set_title('Adaline - Learning rate = 0.01')\n",
    "\n",
    "#eta=0.0001\n",
    "ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X,y)\n",
    "ax[1].plot(range(1, len(ada2.cost_) + 1), np.log10(ada2.cost_), marker='o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('log(Sum-Squared-Error)')\n",
    "ax[1].set_title('Adaline - Learning rate = 0.0001')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
